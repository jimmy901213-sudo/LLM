import streamlit as st
import os
from langchain_ollama.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain_ollama.embeddings import OllamaEmbeddings
from pydantic import BaseModel, Field
from typing import List, Dict

# --- 1. å®šç¾© Pydantic Schema (æˆ‘å€‘çš„çµæ§‹åŒ–è¼¸å‡º) ---
# é€™å€‹ Schema å°‡å¼·åˆ¶ Llama 3 ä»¥æˆ‘å€‘æƒ³è¦çš„ JSON æ ¼å¼å›æ‡‰ [3, 4]
class MarketingContent(BaseModel):
    product_name: str = Field(description="ç”¢å“çš„å®˜æ–¹å…¨å")
    catchy_title: str = Field(description="å„ªåŒ–çš„ AIO/SEO æ¨™é¡Œï¼Œä¸è¶…é 60 å€‹å­—ç¬¦")
    experience_paragraph: str = Field(description="E-E-A-T åŒ–çš„ç¬¬ä¸€äººç¨±ä½¿ç”¨ç¶“é©—æ®µè½ï¼Œéœ€çµåˆä¸€å€‹çœŸå¯¦å ´æ™¯")
    features_bullets: List[str] = Field(description="å¾ç”¢å“äº‹å¯¦ä¸­æå–çš„ 3-5 å€‹æ ¸å¿ƒåŠŸèƒ½åˆ—è¡¨")
    semantic_tags: List[str] = Field(description="ç›¸é—œçš„èªç¾©é—œéµå­—å’Œå¯¦é«” (ä¾‹å¦‚ 'æˆ¶å¤–', 'æ´¾å°')")
    qa_pairs: List[Dict[str, str]] = Field(description="2-3 å€‹ Q&A å°ï¼Œæ ¼å¼ç‚º [{'q': '...', 'a': '...'}]")

# --- 2. ç³»çµ±åˆå§‹åŒ– (ç·©å­˜ä»¥æé«˜æ•ˆèƒ½) ---
@st.cache_resource
def load_system():
    # æª¢æŸ¥ ChromaDB æ˜¯å¦å­˜åœ¨
    db_path = "./chroma_db"
    if not os.path.exists(db_path):
        return None, None, None, "ChromaDB ç›®éŒ„æœªæ‰¾åˆ°ã€‚è«‹å…ˆé‹è¡Œ 'create_vectorstore.py'ã€‚"

    try:
        # 1. åˆå§‹åŒ– LLM (çµæ§‹åŒ–è¼¸å‡º)
        # æˆ‘å€‘å°‡ Pydantic æ¨¡å‹ç¶å®šåˆ° Llama 3 [3, 2]
        llm = ChatOllama(model="llama3:8b", temperature=0.1)
        llm_structured = llm.with_structured_output(MarketingContent)
        
        # 2. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        # ç¢ºä¿ `ollama pull nomic-embed-text` å·²ç¶“é‹è¡Œ
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        
        # 3. åŠ è¼‰å‘é‡æ•¸æ“šåº« [5, 6]
        vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
        
        # 4. å‰µå»ºå…©å€‹æª¢ç´¢å™¨ (å¯¦ç¾é›™é‡çŸ¥è­˜åº«æ¶æ§‹)
        retriever_products = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"filter": {"source": "product_db"}, "k": 1} # åƒ…æª¢ç´¢KB-Aï¼Œåªè¿”å›æœ€åŒ¹é…çš„ 1 å€‹ç”¢å“
        )
        retriever_rules = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"filter": {"source": "aio_rules"}, "k": 5} # æª¢ç´¢KB-Bï¼Œè¿”å› 5 æ¢ç›¸é—œè¦å‰‡
        )

        return llm_structured, retriever_products, retriever_rules, None
    except Exception as e:
        return None, None, None, f"ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œï¼Œä¸¦ä¸”æ‚¨å·²æ‹‰å– 'llama3:8b' å’Œ 'nomic-embed-text' æ¨¡å‹ã€‚"

# --- 3. Streamlit ç•Œé¢ [7] ---
st.set_page_config(layout="wide")
st.title("ğŸš€ AIO/SEO è¡ŒéŠ·å…§å®¹ç”Ÿæˆå™¨ (æœ¬åœ°ç‰ˆ)")
st.caption("ä¸€å€‹ä½¿ç”¨ Llama 3ã€RAG å’Œ Pydantic çš„é›¶é ç®—å¤§å­¸ç”Ÿå°ˆæ¡ˆ")

# åŠ è¼‰ç³»çµ±
structured_llm, prod_retriever, rule_retriever, error_msg = load_system()

if error_msg:
    st.error(error_msg)
else:
    st.sidebar.header("è¨­ç½®")
    product_query = st.sidebar.text_input("è¼¸å…¥ç”¢å“åç¨± (ä¾‹å¦‚: X-100 éŸ³ç®± æˆ– Z-500 è€³æ©Ÿ)")
    
    if st.sidebar.button("ç”Ÿæˆ AIO/SEO å„ªåŒ–å…§å®¹"):
        if not product_query:
            st.sidebar.error("è«‹è¼¸å…¥ç”¢å“åç¨±")
        else:
            with st.spinner("ç³»çµ±æ­£åœ¨æ€è€ƒ... (æœ¬åœ° Llama 3 8B é‹è¡Œä¸­ï¼Œè«‹è€å¿ƒç­‰å¾…...)"):
                
                # --- RAG æ ¸å¿ƒé‚è¼¯ ---
                
                # 1. æª¢ç´¢ KB-A (ç”¢å“)
                product_context_docs = prod_retriever.invoke(product_query)
                product_context = "\n---\n".join([doc.page_content for doc in product_context_docs])
                
                # 2. æª¢ç´¢ KB-B (è¦å‰‡) - æŸ¥è©¢æ˜¯å›ºå®šçš„ï¼Œæˆ‘å€‘éœ€è¦æ‰€æœ‰ç›¸é—œè¦å‰‡
                rule_context_docs = rule_retriever.invoke("æ‰€æœ‰ AIO/SEO/E-E-A-T è¡ŒéŠ·è¦å‰‡")
                rule_context = "\n---\n".join([doc.page_content for doc in rule_context_docs])
                
                # 3. å‰µå»º RAG æç¤ºè© [8, 9]
                # (å¾ Pydantic Schema ä¸­ç²å– JSON Schema æè¿°ä»¥æŒ‡å° LLM [3])
                json_schema_description = MarketingContent.model_json_schema()
                
                prompt = f"""
                ä½ æ˜¯ä¸€åå°ˆæ¥­çš„é›»å•†è¡ŒéŠ·å…§å®¹æ’°å¯«å°ˆå®¶ï¼Œç²¾é€š AIO (AI å„ªåŒ–) å’Œ Google E-E-A-T è¦å‰‡ã€‚

                **ä½ çš„ä»»å‹™**ï¼š
                æ ¹æ“šä¸‹æ–¹æä¾›çš„ã€Œç”¢å“äº‹å¯¦ã€å’Œã€Œè¡ŒéŠ·è¦å‰‡ã€ï¼Œç‚ºè©²ç”¢å“ç”Ÿæˆå„ªåŒ–çš„è¡ŒéŠ·å…§å®¹ã€‚
                ä½ å¿…é ˆåš´æ ¼æŒ‰ç…§ã€Œè¼¸å‡ºæ ¼å¼ã€è¦æ±‚ï¼Œåƒ…è¿”å›ä¸€å€‹ JSON å°è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡‹æˆ–é¡å¤–çš„æ–‡æœ¬ã€‚

                ---
                [ä¸Šä¸‹æ–‡ 1ï¼šç”¢å“äº‹å¯¦]
                {product_context}
                ---
                [ä¸Šä¸‹æ–‡ 2ï¼šè¡ŒéŠ·è¦å‰‡]
                {rule_context}
                ---
                [ç”¨æˆ¶æŸ¥è©¢]: "ç‚º {product_query} ç”Ÿæˆå®Œæ•´çš„ AIO å„ªåŒ–è¡ŒéŠ·å…§å®¹"
                ---
                [è¼¸å‡ºæ ¼å¼]: è«‹åš´æ ¼éµå¾ªæ­¤ JSON Schema: {json_schema_description}
                ---
                """
                
                # --- Streamlit ä½œç‚ºèª¿è©¦å™¨ ---
                with st.expander("ğŸ” é»æ­¤æŸ¥çœ‹ RAG ç³»çµ±çš„ã€æ€è€ƒéç¨‹ã€"):
                    st.subheader("æª¢ç´¢åˆ°çš„ [ç”¢å“äº‹å¯¦]:")
                    st.text(product_context)
                    st.subheader("æª¢ç´¢åˆ°çš„ [è¡ŒéŠ·è¦å‰‡]:")
                    st.text(rule_context)
                    st.subheader("å®Œæ•´çš„ RAG æç¤ºè© (ç™¼é€çµ¦ Llama 3):")
                    st.text(prompt)
                
                # 4. èª¿ç”¨çµæ§‹åŒ– LLM 
                try:
                    #.invoke() æœƒè¿”å›ä¸€å€‹ Pydantic å°è±¡ï¼Œè€Œä¸æ˜¯åŸå§‹å­—ç¬¦ä¸²
                    response_obj = structured_llm.invoke(prompt) 
                    
                    st.subheader(f"âœ… ç‚º {response_obj.product_name} ç”Ÿæˆçš„å…§å®¹ï¼š")
                    
                    st.markdown(f"### {response_obj.catchy_title}")
                    st.divider()
                    
                    st.markdown("#### E-E-A-T ç¶“é©—æ®µè½:")
                    st.markdown(response_obj.experience_paragraph)
                    
                    st.markdown("#### æ ¸å¿ƒåŠŸèƒ½ (AIO åˆ—è¡¨):")
                    st.markdown("\n".join(f"- {item}" for item in response_obj.features_bullets))
                    
                    st.markdown("#### Q&A éƒ¨åˆ†:")
                    for pair in response_obj.qa_pairs:
                        st.markdown(f"**Q: {pair['q']}**")
                        st.markdown(f"A: {pair['a']}")
                    
                    st.markdown("#### èªç¾©æ¨™ç±¤:")
                    st.markdown(", ".join(response_obj.semantic_tags))
                        
                    st.markdown("---")
                    st.subheader("åŸå§‹ JSON è¼¸å‡º (ç”¨æ–¼ API):")
                    st.json(response_obj.model_dump_json())
                    
                except Exception as e:
                    st.error(f"æœ¬åœ° LLM è¼¸å‡ºéŒ¯èª¤ï¼š{e}")
                    st.error("Llama 3 8B æœªèƒ½æ­£ç¢ºç”Ÿæˆ Pydantic çµæ§‹ã€‚è«‹å˜—è©¦é‡å•Ÿ Ollama æˆ–æª¢æŸ¥æç¤ºè©ã€‚")